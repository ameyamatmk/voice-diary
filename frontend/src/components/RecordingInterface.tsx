'use client'

import React, { useState, useRef, useCallback, useEffect } from 'react'
import { Mic, Square, Play, Pause, MessageSquare } from 'lucide-react'
import '../types/speech-recognition'

type RecordingState = 'idle' | 'recording' | 'paused' | 'processing'

interface RecordingInterfaceProps {
  onRecordingComplete?: (audioBlob: Blob) => void
  disabled?: boolean
  enableRealtimeTranscription?: boolean
}

export const RecordingInterface: React.FC<RecordingInterfaceProps> = ({
  onRecordingComplete,
  disabled = false,
  enableRealtimeTranscription = true
}) => {
  const [recordingState, setRecordingState] = useState<RecordingState>('idle')
  const [recordingTime, setRecordingTime] = useState(0)
  const [audioLevel, setAudioLevel] = useState(0)
  const [realtimeText, setRealtimeText] = useState('')
  const [isListening, setIsListening] = useState(false)
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const streamRef = useRef<MediaStream | null>(null)
  const timerRef = useRef<NodeJS.Timeout | null>(null)
  const animationRef = useRef<number | null>(null)
  const recognitionRef = useRef<SpeechRecognition | null>(null)

  // Web Speech APIã®åˆæœŸåŒ–
  useEffect(() => {
    if (!enableRealtimeTranscription) return
    
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition
      const recognition = new SpeechRecognition()
      
      recognition.continuous = true
      recognition.interimResults = true
      recognition.lang = 'ja-JP'
      
      recognition.onstart = () => {
        setIsListening(true)
      }
      
      recognition.onresult = (event) => {
        let interimTranscript = ''
        let finalTranscript = ''
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript
          if (event.results[i].isFinal) {
            finalTranscript += transcript
          } else {
            interimTranscript += transcript
          }
        }
        
        // ç¢ºå®šãƒ†ã‚­ã‚¹ãƒˆ + ä»®ç¢ºå®šãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
        setRealtimeText(prev => {
          const lines = prev.split('\n')
          const lastLine = lines[lines.length - 1]
          
          if (finalTranscript) {
            // ç¢ºå®šãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
            return prev + finalTranscript + (interimTranscript ? ' ' + interimTranscript : '')
          } else {
            // ä»®ç¢ºå®šãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®å ´åˆã¯æœ€çµ‚è¡Œã‚’æ›´æ–°
            const baseText = lines.slice(0, -1).join('\n')
            return baseText + (baseText ? '\n' : '') + interimTranscript
          }
        })
      }
      
      recognition.onerror = (event) => {
        console.log('Speech recognition error:', event.error)
        setIsListening(false)
      }
      
      recognition.onend = () => {
        setIsListening(false)
      }
      
      recognitionRef.current = recognition
    }
    
    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop()
      }
    }
  }, [enableRealtimeTranscription])

  const formatTime = useCallback((seconds: number) => {
    const mins = Math.floor(seconds / 60)
    const secs = seconds % 60
    return `${mins}:${secs.toString().padStart(2, '0')}`
  }, [])

  const startSpeechRecognition = useCallback(() => {
    if (enableRealtimeTranscription && recognitionRef.current && !isListening) {
      setRealtimeText('')
      try {
        recognitionRef.current.start()
      } catch (error) {
        console.log('Speech recognition start error:', error)
      }
    }
  }, [enableRealtimeTranscription, isListening])

  const stopSpeechRecognition = useCallback(() => {
    if (recognitionRef.current && isListening) {
      recognitionRef.current.stop()
      setRealtimeText('')
    }
  }, [isListening])

  const startRecording = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 44100,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        } 
      })
      
      streamRef.current = stream
      
      // éŸ³å£°ãƒ¬ãƒ™ãƒ«è¡¨ç¤ºç”¨ã®AudioContextè¨­å®š
      const audioContext = new AudioContext()
      const analyser = audioContext.createAnalyser()
      const source = audioContext.createMediaStreamSource(stream)
      
      analyser.fftSize = 256
      source.connect(analyser)
      
      audioContextRef.current = audioContext
      analyserRef.current = analyser
      
      // MediaRecorderè¨­å®š
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      })
      
      const chunks: Blob[] = []
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunks.push(event.data)
        }
      }
      
      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(chunks, { type: 'audio/webm' })
        onRecordingComplete?.(audioBlob)
        setRecordingState('idle')
        setRecordingTime(0)
        setAudioLevel(0)
      }
      
      mediaRecorderRef.current = mediaRecorder
      mediaRecorder.start()
      setRecordingState('recording')
      
      // Web Speech APIé–‹å§‹
      startSpeechRecognition()
      
      // ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => {
          if (prev >= 600) { // 10åˆ†åˆ¶é™
            stopRecording()
            return prev
          }
          return prev + 1
        })
      }, 1000)
      
      // éŸ³å£°ãƒ¬ãƒ™ãƒ«ç›£è¦–é–‹å§‹
      updateAudioLevel()
      
    } catch (error) {
      console.error('éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼:', error)
      alert('ãƒã‚¤ã‚¯ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚')
    }
  }, [onRecordingComplete, startSpeechRecognition])

  const updateAudioLevel = useCallback(() => {
    if (!analyserRef.current) return
    
    const bufferLength = analyserRef.current.frequencyBinCount
    const dataArray = new Uint8Array(bufferLength)
    analyserRef.current.getByteTimeDomainData(dataArray)
    
    // RMS (Root Mean Square) è¨ˆç®—ã§ã‚ˆã‚Šæ­£ç¢ºãªéŸ³å£°ãƒ¬ãƒ™ãƒ«ã‚’å–å¾—
    let sum = 0
    for (let i = 0; i < bufferLength; i++) {
      const sample = (dataArray[i] - 128) / 128 // -1ã‹ã‚‰1ã®ç¯„å›²ã«æ­£è¦åŒ–
      sum += sample * sample
    }
    const rms = Math.sqrt(sum / bufferLength)
    
    // éŸ³å£°ãƒ¬ãƒ™ãƒ«ã‚’0-1ã®ç¯„å›²ã«èª¿æ•´ï¼ˆæ„Ÿåº¦ã‚’ä¸Šã’ã‚‹ï¼‰
    const level = Math.min(rms * 5, 1)
    setAudioLevel(level)
    
    if (recordingState === 'recording') {
      animationRef.current = requestAnimationFrame(updateAudioLevel)
    }
  }, [recordingState])

  const pauseRecording = useCallback(() => {
    if (mediaRecorderRef.current && recordingState === 'recording') {
      mediaRecorderRef.current.pause()
      setRecordingState('paused')
      
      // Web Speech APIåœæ­¢
      stopSpeechRecognition()
      
      if (timerRef.current) {
        clearInterval(timerRef.current)
      }
      if (animationRef.current) {
        cancelAnimationFrame(animationRef.current)
      }
    }
  }, [recordingState, stopSpeechRecognition])

  const resumeRecording = useCallback(() => {
    if (mediaRecorderRef.current && recordingState === 'paused') {
      mediaRecorderRef.current.resume()
      setRecordingState('recording')
      
      // Web Speech APIå†é–‹
      startSpeechRecognition()
      
      // ã‚¿ã‚¤ãƒãƒ¼å†é–‹
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => {
          if (prev >= 600) {
            stopRecording()
            return prev
          }
          return prev + 1
        })
      }, 1000)
      
      // éŸ³å£°ãƒ¬ãƒ™ãƒ«ç›£è¦–å†é–‹
      updateAudioLevel()
    }
  }, [recordingState, updateAudioLevel, startSpeechRecognition])

  const stopRecording = useCallback(() => {
    if (mediaRecorderRef.current) {
      mediaRecorderRef.current.stop()
      setRecordingState('processing')
    }
    
    // Web Speech APIåœæ­¢ãƒ»ã‚¯ãƒªã‚¢
    stopSpeechRecognition()
    
    // ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
    }
    if (timerRef.current) {
      clearInterval(timerRef.current)
    }
    if (animationRef.current) {
      cancelAnimationFrame(animationRef.current)
    }
    if (audioContextRef.current) {
      audioContextRef.current.close()
    }
  }, [stopSpeechRecognition])

  const getRecordingButtonStyle = () => {
    switch (recordingState) {
      case 'recording':
        return 'bg-recording animate-recording-pulse shadow-lg hover:bg-recording'
      case 'paused':
        return 'bg-warning hover:bg-warning/80'
      case 'processing':
        return 'bg-accent-primary animate-spin cursor-not-allowed'
      default:
        return 'bg-accent-primary hover:bg-accent-secondary'
    }
  }

  const getRecordingButtonIcon = () => {
    switch (recordingState) {
      case 'recording':
        return <Square className="w-6 h-6" />
      case 'paused':
        return <Play className="w-6 h-6" />
      case 'processing':
        return <div className="w-6 h-6 border-2 border-white border-t-transparent rounded-full" />
      default:
        return <Mic className="w-6 h-6" />
    }
  }

  const renderAudioLevelBars = () => {
    const bars = Array.from({ length: 20 }, (_, i) => {
      const barThreshold = i / 20 // 0ã‹ã‚‰1ã®ç¯„å›²
      const isActive = audioLevel > barThreshold
      const height = isActive ? Math.max(20, audioLevel * 80) : 10 // æœ€å°20%, æœ€å¤§80%
      
      return (
        <div
          key={i}
          className={`w-2 rounded-full transition-all duration-100 ${
            isActive ? 'bg-success' : 'bg-border'
          }`}
          style={{ height: `${height}%` }}
        />
      )
    })
    
    return (
      <div className="flex items-end justify-center gap-1 h-24 w-full bg-bg-tertiary rounded-lg mb-6 p-4">
        {recordingState === 'recording' ? bars : (
          <div className="text-text-muted text-sm flex items-center justify-center h-full">
            éŸ³å£°ãƒ¬ãƒ™ãƒ«è¡¨ç¤º
          </div>
        )}
      </div>
    )
  }

  return (
    <div className="bg-bg-secondary rounded-2xl p-8 shadow-lg border border-border">
      <div className="text-center">
        <h2 className="text-2xl font-semibold text-text-primary mb-6">
          éŸ³å£°éŒ²éŸ³
        </h2>
        
        {/* ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—è¡¨ç¤º */}
        {enableRealtimeTranscription && (recordingState === 'recording' || recordingState === 'paused') && (
          <div className="mb-6">
            <div className="flex items-center justify-center gap-2 mb-3">
              <MessageSquare className="w-4 h-4 text-text-muted" />
              <span className="text-sm font-medium text-text-muted">
                ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—
              </span>
              {isListening && (
                <div className="w-2 h-2 bg-success rounded-full animate-pulse" />
              )}
            </div>
            <div className="bg-bg-primary rounded-lg p-4 min-h-[120px] max-h-[200px] overflow-y-auto text-left border border-border">
              {realtimeText ? (
                <p className="text-text-primary text-sm leading-relaxed whitespace-pre-wrap">
                  {realtimeText}
                </p>
              ) : (
                <p className="text-text-muted text-sm italic text-center flex items-center justify-center h-full">
                  {isListening ? 'éŸ³å£°ã‚’èªè­˜ä¸­...' : 'æ–‡å­—èµ·ã“ã—å¾…æ©Ÿä¸­'}
                </p>
              )}
            </div>
          </div>
        )}
        
        {/* éŸ³å£°ãƒ¬ãƒ™ãƒ«è¡¨ç¤º */}
        {renderAudioLevelBars()}
        
        {/* éŒ²éŸ³æ™‚é–“ */}
        <div className="text-3xl font-mono text-text-primary mb-6">
          {formatTime(recordingTime)}
        </div>
        
        {/* éŒ²éŸ³åˆ¶å¾¡ãƒœã‚¿ãƒ³ */}
        <div className="flex items-center justify-center gap-4 mb-4">
          <button
            onClick={
              recordingState === 'idle' ? startRecording :
              recordingState === 'recording' ? stopRecording :
              recordingState === 'paused' ? resumeRecording :
              undefined
            }
            disabled={recordingState === 'processing' || disabled}
            className={`
              w-16 h-16 rounded-full text-white font-semibold
              flex items-center justify-center
              touch-target focus-visible transition-all duration-200
              ${getRecordingButtonStyle()}
            `}
          >
            {getRecordingButtonIcon()}
          </button>
          
          {recordingState === 'recording' && (
            <button
              onClick={pauseRecording}
              className="w-12 h-12 rounded-full bg-bg-tertiary text-text-primary hover:bg-border touch-target focus-visible transition-all duration-200 flex items-center justify-center"
            >
              <Pause className="w-5 h-5" />
            </button>
          )}
        </div>
        
        {/* çŠ¶æ…‹è¡¨ç¤º */}
        <div className="text-sm text-text-muted">
          {recordingState === 'idle' && 'éŒ²éŸ³ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦é–‹å§‹'}
          {recordingState === 'recording' && (
            <div className="flex flex-col items-center gap-1">
              <span>éŒ²éŸ³ä¸­... (æœ€å¤§10åˆ†)</span>
              {enableRealtimeTranscription && (
                <span className="text-xs">
                  {isListening ? 'ğŸ¤ éŸ³å£°èªè­˜ä¸­' : 'ğŸ”‡ éŸ³å£°èªè­˜å¾…æ©Ÿä¸­'}
                </span>
              )}
            </div>
          )}
          {recordingState === 'paused' && 'ä¸€æ™‚åœæ­¢ä¸­'}
          {recordingState === 'processing' && 'å‡¦ç†ä¸­...'}
        </div>
        
        {recordingTime >= 600 && (
          <div className="mt-2 text-sm text-warning">
            æœ€å¤§éŒ²éŸ³æ™‚é–“ã«é”ã—ã¾ã—ãŸ
          </div>
        )}
      </div>
    </div>
  )
}