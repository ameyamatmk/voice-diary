'use client'

import React, { useState, useRef, useCallback, useEffect } from 'react'
import { Mic, Square, Play, Pause, MessageSquare } from 'lucide-react'
import '../types/speech-recognition'
import { useFlashMessage } from './FlashMessage'

type RecordingState = 'idle' | 'recording' | 'paused' | 'processing'

interface RecordingInterfaceProps {
  onRecordingComplete?: (audioBlob: Blob) => void
  disabled?: boolean
  enableRealtimeTranscription?: boolean
}

export const RecordingInterface: React.FC<RecordingInterfaceProps> = ({
  onRecordingComplete,
  disabled = false,
  enableRealtimeTranscription = true
}) => {
  const [recordingState, setRecordingState] = useState<RecordingState>('idle')
  const [recordingTime, setRecordingTime] = useState(0)
  const [realtimeText, setRealtimeText] = useState('')
  const [isListening, setIsListening] = useState(false)

  const { showError, showWarning, FlashMessageContainer } = useFlashMessage()

  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const streamRef = useRef<MediaStream | null>(null)
  const timerRef = useRef<NodeJS.Timeout | null>(null)
  const recognitionRef = useRef<SpeechRecognition | null>(null)
  const recordingTimeRef = useRef<number>(0)

  // Web Speech APIã®åˆæœŸåŒ–
  useEffect(() => {
    if (!enableRealtimeTranscription) return

    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition
      const recognition = new SpeechRecognition()

      recognition.continuous = true
      recognition.interimResults = true
      recognition.lang = 'ja-JP'

      recognition.onstart = () => {
        setIsListening(true)
      }

      recognition.onresult = (event) => {
        let interimTranscript = ''
        let finalTranscript = ''

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript
          if (event.results[i].isFinal) {
            finalTranscript += transcript
          } else {
            interimTranscript += transcript
          }
        }

        // ç¢ºå®šãƒ†ã‚­ã‚¹ãƒˆ + ä»®ç¢ºå®šãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
        setRealtimeText(prev => {
          const lines = prev.split('\n')
          const lastLine = lines[lines.length - 1]

          if (finalTranscript) {
            // ç¢ºå®šãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
            return prev + finalTranscript + (interimTranscript ? ' ' + interimTranscript : '')
          } else {
            // ä»®ç¢ºå®šãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®å ´åˆã¯æœ€çµ‚è¡Œã‚’æ›´æ–°
            const baseText = lines.slice(0, -1).join('\n')
            return baseText + (baseText ? '\n' : '') + interimTranscript
          }
        })
      }

      recognition.onerror = (event) => {
        console.log('Speech recognition error:', event.error)
        setIsListening(false)
      }

      recognition.onend = () => {
        setIsListening(false)
      }

      recognitionRef.current = recognition
    }

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop()
      }
    }
  }, [enableRealtimeTranscription])

  const formatTime = useCallback((seconds: number) => {
    const mins = Math.floor(seconds / 60)
    const secs = seconds % 60
    return `${mins}:${secs.toString().padStart(2, '0')}`
  }, [])

  const startSpeechRecognition = useCallback(() => {
    if (enableRealtimeTranscription && recognitionRef.current && !isListening) {
      setRealtimeText('')
      try {
        recognitionRef.current.start()
      } catch (error) {
        console.log('Speech recognition start error:', error)
      }
    }
  }, [enableRealtimeTranscription, isListening])

  const stopSpeechRecognition = useCallback(() => {
    if (recognitionRef.current && isListening) {
      recognitionRef.current.stop()
      setRealtimeText('')
    }
  }, [isListening])

  const startRecording = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 44100,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        }
      })

      streamRef.current = stream

      // MediaRecorderè¨­å®š
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      })

      const chunks: Blob[] = []

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunks.push(event.data)
        }
      }

      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(chunks, { type: 'audio/webm' })

        // éŒ²éŸ³æ™‚é–“ãŒ10ç§’æœªæº€ã®å ´åˆã¯ä¿å­˜ã—ãªã„
        if (recordingTimeRef.current < 10) {
          showError(
            'éŒ²éŸ³æ™‚é–“ãŒçŸ­ã™ãã¾ã™ã€‚10ç§’ä»¥ä¸ŠéŒ²éŸ³ã—ã¦ãã ã•ã„ã€‚',
            'éŒ²éŸ³æ™‚é–“ä¸è¶³'
          )
          setRecordingState('idle')
          setRecordingTime(0)
          recordingTimeRef.current = 0
          return
        }

        onRecordingComplete?.(audioBlob)
        setRecordingState('idle')
        setRecordingTime(0)
        recordingTimeRef.current = 0
      }

      mediaRecorderRef.current = mediaRecorder
      mediaRecorder.start()
      setRecordingState('recording')

      // Web Speech APIé–‹å§‹
      startSpeechRecognition()

      // ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => {
          const newTime = prev + 1
          recordingTimeRef.current = newTime
          if (newTime >= 600) { // 10åˆ†åˆ¶é™
            stopRecording()
            return newTime
          }
          return newTime
        })
      }, 1000)

    } catch (error) {
      console.error('éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼:', error)
      showError(
        'ãƒã‚¤ã‚¯ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚',
        'ãƒã‚¤ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼'
      )
    }
  }, [onRecordingComplete, startSpeechRecognition])

  const pauseRecording = useCallback(() => {
    if (mediaRecorderRef.current && recordingState === 'recording') {
      mediaRecorderRef.current.pause()
      setRecordingState('paused')

      // Web Speech APIåœæ­¢
      stopSpeechRecognition()

      if (timerRef.current) {
        clearInterval(timerRef.current)
      }
    }
  }, [recordingState, stopSpeechRecognition])

  const resumeRecording = useCallback(() => {
    if (mediaRecorderRef.current && recordingState === 'paused') {
      mediaRecorderRef.current.resume()
      setRecordingState('recording')

      // Web Speech APIå†é–‹
      startSpeechRecognition()

      // ã‚¿ã‚¤ãƒãƒ¼å†é–‹
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => {
          const newTime = prev + 1
          recordingTimeRef.current = newTime
          if (newTime >= 600) {
            stopRecording()
            return newTime
          }
          return newTime
        })
      }, 1000)
    }
  }, [recordingState, startSpeechRecognition])

  const stopRecording = useCallback(() => {
    if (mediaRecorderRef.current) {
      mediaRecorderRef.current.stop()
      setRecordingState('processing')
    }

    // Web Speech APIåœæ­¢ãƒ»ã‚¯ãƒªã‚¢
    stopSpeechRecognition()

    // ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
    }
    if (timerRef.current) {
      clearInterval(timerRef.current)
    }
  }, [stopSpeechRecognition])

  const getRecordingButtonStyle = () => {
    switch (recordingState) {
      case 'recording':
        return 'bg-recording animate-recording-pulse shadow-lg hover:bg-recording'
      case 'paused':
        return 'bg-warning hover:bg-warning/80'
      case 'processing':
        return 'bg-accent-primary animate-spin cursor-not-allowed'
      default:
        return 'bg-accent-primary hover:bg-accent-secondary'
    }
  }

  const getRecordingButtonIcon = () => {
    switch (recordingState) {
      case 'recording':
        return <Square className="w-6 h-6" />
      case 'paused':
        return <Play className="w-6 h-6" />
      case 'processing':
        return <div className="w-6 h-6 border-2 border-white border-t-transparent rounded-full" />
      default:
        return <Mic className="w-6 h-6" />
    }
  }

  const renderRecordingStatus = () => {
    return (
      <div className="flex items-center justify-center h-24 w-full bg-bg-tertiary rounded-lg mb-6 p-4 border border-border">
        <div className="text-center">
          {recordingState === 'idle' && (
            <div className="text-text-muted text-sm">
              éŒ²éŸ³ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦é–‹å§‹
            </div>
          )}
          {recordingState === 'recording' && (
            <div className="text-center">
              <div className="w-4 h-4 bg-recording rounded-full animate-pulse mx-auto mb-2"></div>
              <div className="text-recording text-sm font-medium">éŒ²éŸ³ä¸­...</div>
            </div>
          )}
          {recordingState === 'paused' && (
            <div className="text-center">
              <div className="w-4 h-4 bg-warning rounded-full mx-auto mb-2"></div>
              <div className="text-warning text-sm font-medium">ä¸€æ™‚åœæ­¢ä¸­</div>
            </div>
          )}
          {recordingState === 'processing' && (
            <div className="text-center">
              <div className="w-4 h-4 border-2 border-accent-primary border-t-transparent rounded-full animate-spin mx-auto mb-2"></div>
              <div className="text-accent-primary text-sm font-medium">å‡¦ç†ä¸­...</div>
            </div>
          )}
        </div>
      </div>
    )
  }

  return (
    <>
      <FlashMessageContainer />
      <div className="bg-bg-secondary rounded-2xl p-8 shadow-lg border border-border">
        <div className="text-center">
          <h2 className="text-2xl font-semibold text-text-primary mb-6">
            éŸ³å£°éŒ²éŸ³
          </h2>

          {/* ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—è¡¨ç¤º */}
          {enableRealtimeTranscription && (
            <div className="mb-6">
              <div className="flex items-center justify-center gap-2 mb-3">
                <MessageSquare className="w-4 h-4 text-text-muted" />
                <span className="text-sm font-medium text-text-muted">
                  ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—
                </span>
                {isListening && (
                  <div className="w-2 h-2 bg-success rounded-full animate-pulse" />
                )}
              </div>
              <div className="bg-bg-primary rounded-lg p-4 min-h-[120px] max-h-[200px] overflow-y-auto text-left border border-border">
                {realtimeText ? (
                  <p className="text-text-primary text-sm leading-relaxed whitespace-pre-wrap">
                    {realtimeText}
                  </p>
                ) : (
                  <p className="text-text-muted text-sm italic text-center flex items-center justify-center h-full">
                    {isListening ? 'éŸ³å£°ã‚’èªè­˜ä¸­...' : 'æ–‡å­—èµ·ã“ã—å¾…æ©Ÿä¸­'}
                  </p>
                )}
              </div>
            </div>
          )}

          {/* éŒ²éŸ³æ™‚é–“ */}
          <div className="text-3xl font-mono text-text-primary mb-6">
            {formatTime(recordingTime)}
          </div>

          {/* éŒ²éŸ³åˆ¶å¾¡ãƒœã‚¿ãƒ³ */}
          <div className="flex items-center justify-center gap-4 mb-4">
            <button
              onClick={
                recordingState === 'idle' ? startRecording :
                  recordingState === 'recording' ? stopRecording :
                    recordingState === 'paused' ? resumeRecording :
                      undefined
              }
              disabled={recordingState === 'processing' || disabled}
              className={`
              w-16 h-16 rounded-full text-white font-semibold
              flex items-center justify-center
              touch-target focus-visible transition-all duration-200
              ${getRecordingButtonStyle()}
            `}
            >
              {getRecordingButtonIcon()}
            </button>

            {recordingState === 'recording' && (
              <button
                onClick={pauseRecording}
                className="w-12 h-12 rounded-full bg-bg-tertiary text-text-primary hover:bg-border touch-target focus-visible transition-all duration-200 flex items-center justify-center"
              >
                <Pause className="w-5 h-5" />
              </button>
            )}
          </div>

          {/* è¿½åŠ æƒ…å ±è¡¨ç¤º */}
          <div className="text-sm text-text-muted">
            {recordingState === 'idle' && (
              <div className="text-center">
                <span>æœ€ä½10ç§’ã€æœ€å¤§10åˆ†ã¾ã§éŒ²éŸ³å¯èƒ½</span>
              </div>
            )}
            {recordingState === 'recording' && (
              <div className="flex flex-col items-center gap-1">
                <span className={recordingTime < 10 ? "text-warning" : ""}>
                  {recordingTime < 10 ? `ã‚ã¨${10 - recordingTime}ç§’ã§ä¿å­˜å¯èƒ½` : 'æœ€å¤§10åˆ†ã¾ã§éŒ²éŸ³å¯èƒ½'}
                </span>
                {enableRealtimeTranscription && (
                  <span className="text-xs">
                    {isListening ? 'ğŸ¤ éŸ³å£°èªè­˜ä¸­' : 'ğŸ”‡ éŸ³å£°èªè­˜å¾…æ©Ÿä¸­'}
                  </span>
                )}
              </div>
            )}
          </div>

          {recordingTime >= 600 && (
            <div className="mt-2 text-sm text-warning">
              æœ€å¤§éŒ²éŸ³æ™‚é–“ã«é”ã—ã¾ã—ãŸ
            </div>
          )}
        </div>
      </div>
    </>
  )
}